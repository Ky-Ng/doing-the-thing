{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Doing The Thing: From Mechanical Turk to Mech Interp","text":"<p><sub> For motivation/momentum purposes, current consecutive streak: 15 days! </sub></p>"},{"location":"#what-is-this","title":"What is this?","text":"<p>Here it goes! I'm Kyle, a 4th year Computational Linguistics student at USC. This is the start of my AI Safety journey!</p> <p>I learned about AI Safety from the Seattle Llama4 Hackathon on June 21st, 2025 where I learned of AI 2027. After finishing an awesome summer of engineering, I realized the problems which excite me the most lie at the crossroads of engineering and science (computation and linguistics). </p> <p>The urgent need for understanding increasingly capable AI models coupled with a burning passion for working at the interdisciplinary intersection of NLP, linguistics, and engineering at scale has sharpened my goal: to become an AI Safety researcher in Mechanistic Interpretability.</p>"},{"location":"#working-backwards","title":"Working Backwards","text":"<p>Sometimes (often) I get analysis paralysis or want to wait for the perfect {time, situation, background, preparation} to start which makes it difficult to get into pursuing my goals (and dreams). So this time around, I know my goal to become an AI Safety/Mech Interp researcher! After finding David Quarel's do the thing I decided that this site is a place where I will keep myself acountable for doing the thing.</p> <ul> <li>Doing --&gt; Working through math problems, reading papers, writing down lists of possible intersections of linguistics and NLP</li> <li>The Thing --&gt; Any of the above for at least 1 hour every day, with consistent (though not perfect) progress.</li> </ul> <p></p>"},{"location":"#what-is-dlog","title":"What is dlog?","text":"<p>I am starting this daily log or dlog where every day I will document my progress. I hope that the daily act of documenting will make me more resilient and help prove to myself how badly I want to be an interpetability researcher. With the help of AI, a macro pulls the daily logs into the summary you see below:</p> Extra Stuff (click me)"},{"location":"#wait-whats-computational-linguistics","title":"Wait What's Computational Linguistics?","text":"<p>As a Computational Linguistics student, I see Computational Linguistics as three parts:</p> <ol> <li>Linguistics = study of human language processing / cognition</li> <li>Mechanistic Interpretability = study of LLM language processing / cognition</li> <li>Computational Linguistics = Interdisciplinary approach to studying LLM language processing </li> </ol> <p>The Computational Linguistics topics that pull me at 9.8 m / s^2 are concepts like Information Theory and Probabilistic Phonology in addition to Theoretical Machine Learning and NLP.</p>"},{"location":"#what-does-ai-safety-and-mechanistic-interpretability-mean-to-me","title":"What does AI Safety and Mechanistic Interpretability mean to me?","text":"<ul> <li>I hope that having deep knowledge in both the fields of linguistics and ML/NLP can help me build a more holistic understanding of LLM cognition and language processing. </li> <li>I see Mechanistic Interpretability as a sort of psycholinguistics (the study of real-time processing of language) for LLMs. </li> <li>Furthermore, I see Mechanistic Interpretability as a foundational basis for understanding AI systems. Perhaps understanding models (such as like biological organisms) can support the other branches of AI Safety (alignment, control, governance, and more).</li> </ul>"},{"location":"#dlog-15-days-and-counting","title":"dlog: 15 Days and Counting","text":"<p>Total time focused so far: 31 hrs 43 mins throughout 15 days of learning</p> <p>Below are the latest updates (auto-generated). </p>"},{"location":"#latest-entries","title":"Latest entries","text":"<ul> <li> <p>2025-10-31 | 0 hr 7 min | Goal: No Zero Days Mean, Variance, Std Deviation [StatQuest Video] </p> <p><sub>Stats</sub></p> </li> <li> <p>2025-10-30 | 1 hr 15 min | Goal: Lead first AI Safety Session! First time formally introducing AI Safety to Recursion Reading Group; briefly skim Anthropic's Signs of introspection in large language models </p> <p><sub>Recursion Reading Group</sub></p> </li> <li> <p>2025-10-29 | 1 hr 41 min | Goal: Continue Application Work Pt.2 Looking for research/upskilling opportunities </p> <p><sub>Applications</sub></p> </li> <li> <p>2025-10-28 | 0 hr 18 min | Goal: Continue Application Work! Looking for research/upskilling opportunities </p> <p><sub>Applications</sub></p> </li> <li> <p>2025-10-27 | 2 hr 51 min | Goal: Continue Applications and Start StatQuest journey Watch StatQuest on Probability/Gaussian Distributions, Uniform Information Density Writeup </p> <p><sub>Applications \u00b7 Stats \u00b7 Information Theory</sub></p> </li> <li> <p>2025-10-26 | 2 hr 38 min | Goal: Apply to Research Opportunities Continue reading Anthropic's Tracing the thoughts of a large language model </p> <p><sub>Applications</sub></p> </li> <li> <p>2025-10-25 | 1 hr 10 min | Goal: Work on ARENA 7.0 Application Follow Up Reflect on career goals, start reading Anthropic's Tracing the thoughts of a large language model </p> <p><sub>Applications \u00b7 Documentation</sub></p> </li> <li> <p>2025-10-24 | 1 hr 32 min | Goal: Learn Transformer Lens Learning Transformer Lens for Encoder-Based Syntax Probe </p> <p><sub>Syntax \u00b7 Transformer Lens \u00b7 Dyck Language</sub></p> </li> <li> <p>2025-10-23 | 1 hr 33 min | Goal: Finish adding past days and work on ARENA 2.1 Introduction to MDPs, add docs for 2025-10-18 and 2025-10-19 </p> <p><sub>ARENA \u00b7 RL \u00b7 Bandits \u00b7 MDPs \u00b7 documentation</sub></p> </li> <li> <p>2025-10-22 | 2 hr 7 min | Goal: Update Past Days Added page 2025-10-17 </p> </li> </ul>"},{"location":"#todo-list","title":"Todo List","text":"<p>The todo list started getting too beefy and has been moved to its own todo page!</p>"},{"location":"dlog/2025-10-17/","title":"2025-10-17 | And So It Begins","text":"<p>Goal: Start getting into interpretability</p> <p>Summary: Accidentally started this website after finding ARENA and David Quarel's Do The Thing</p> <p>Work sessions</p> In Out 14:00 16:30 <p>Discovered ARENA RL tutorial (2.1) and started DFS (Depth First Search)ing through RL probability notation and basics</p> <p>Specifically, looking to understand this formula here</p> \\[ (s_{t+1} , r_t) \\sim P( \\cdot | s_t, a_t)\\]","tags":[]},{"location":"dlog/2025-10-18/","title":"2025-10-18 | ARENA","text":"<p>Goal: Applied to ARENA 7.0; Probability Prerequisites</p> <p>Summary: Introducing myself to Gaussian Distributions, Integration, and Entropy</p> <p>Work sessions</p> In Out 11:00 12:00 19:00 22:00 22:00 23:59","tags":["Applications","Information Theory","Probability","Calculus"]},{"location":"dlog/2025-10-18/#learning","title":"Learning","text":"<ol> <li>Work on understanding how Gaussian Distributions and Z-scores work</li> <li>Review of how to take an integral, applied to finding the probability density between two points</li> <li>Look into Shannon 1948 Information Theory PART I: DISCRETE NOISELESS SYSTEMS</li> </ol>","tags":["Applications","Information Theory","Probability","Calculus"]},{"location":"dlog/2025-10-18/#application","title":"Application","text":"<p>Applied to ARENA 7.0. The application was really fun since I learned about sandbagging and scheming. Since I am new to these concepts I wish I had more time to work on reading the papers/responding to the questions but most importantly, I did the thing!</p>","tags":["Applications","Information Theory","Probability","Calculus"]},{"location":"dlog/2025-10-19/","title":"2025-10-19 | Shannon","text":"<p>Goal: Shannon 1948 Discrete Noiseless Channel</p> <p>Summary: Multi-disciplinary approach to Interpretability, viewing models through the lens of Entropy</p> <p>Work sessions</p> In Out 19:00 23:00","tags":["Information Theory"]},{"location":"dlog/2025-10-19/#why-information-theory","title":"Why Information Theory","text":"<p>Shannon 1948 is a seminal text on the way scientists/engineers view how computer's transmit, store, and produce information.</p> <p>While written 77 years ago, concepts like Entropy (in fact, LLMs are trained on Cross-Entropy loss functions!), Mutual Information, and Noise perhaps may lead to greater insight which emerge from model training pressures.</p> <p>Nicely intersected with a reading for my Phonology Course</p>","tags":["Information Theory"]},{"location":"dlog/2025-10-20/","title":"2025-10-20 | Setup","text":"<p>Goal: Create this website</p> <p>Summary: Created website by following MkDocs-Material tutorial</p> <p>Work sessions</p> In Out 12:30 15:00 <p>Followed the Material for MkDocs: Full Tutorial To Build And Deploy Your Docs Portal tutorial and added workflow to automatically deploy to github pages on commits. Also debugged errors introduced by AI generated macro.</p> <p>Watched Neel Nanda \u2013 Mechanistic Interpretability: A Whirlwind Tour. I think I would like to try reproducing the paper Progress measures for grokking via mechanistic interpretability by Nanda et al.</p>","tags":["macros","documentation"]},{"location":"dlog/2025-10-21/","title":"2025-10-21 | Setup Cont.","text":"<p>Goal: Add homepage and latex support</p> <p>Summary: Added documentation for Monday/Tuesday</p> <p>Work sessions</p> In Out 09:00 09:27 11:20 11:40 21:40 22:25 <p>Added the index.md homepage and adding supporting with additional macros and latex.</p> <p>Now there's Latex support too! One of my favorite equations:</p> <p>\\(\\text{softmax}(\\frac{QK^\\top}{\\sqrt{d}})V\\)</p>","tags":["latex","documentation"]},{"location":"dlog/2025-10-22/","title":"2025-10-22 | Doc Updates","text":"<p>Goal: Update Past Days</p> <p>Summary: Added page 2025-10-17</p> <p>Work sessions</p> In Out 20:37 22:44 <ul> <li>Reference page: 2025-10-17</li> </ul>","tags":[]},{"location":"dlog/2025-10-23/","title":"2025-10-23 | MDPs","text":"<p>Goal: Finish adding past days and work on ARENA 2.1</p> <p>Summary: Introduction to MDPs, add docs for 2025-10-18 and 2025-10-19</p> <p>Work sessions</p> In Out 11:35 11:58 18:39 19:49","tags":["ARENA","RL","Bandits","MDPs","documentation"]},{"location":"dlog/2025-10-23/#arena-21-bandits-and-markov-decision-processes","title":"ARENA 2.1 Bandits and Markov Decision Processes","text":"<ul> <li>Quite a lot of math to go through. I think the lecture moves quite fast without too many examples so I will need to start watching the lecture again and likely use AI to make me some practice examples.</li> </ul> <p>Concept to review from Multi-Arm Bandits 1. Expected Values</p> <p>Concept to review from Markov Decision Processes</p> <ol> <li>What is $ \\hat{Q}_t(a) $</li> <li>Discounted Reward $ G_t $ and why geometric sequences are needed with the coefficient $ \\gamma $</li> <li>In the four tuple $ {S, T, A, R} $ what is a the simplex $ \\Delta A $</li> </ol> <p>The goal for tomorrow is to work through some simple problems from MDPs and Expected Values. The goal is to build up an intuition for Bellman's Equation.</p>","tags":["ARENA","RL","Bandits","MDPs","documentation"]},{"location":"dlog/2025-10-23/#updating-docs","title":"Updating docs","text":"<ul> <li>Added dcoumentation for 2025-10-18 and 2025-10-19</li> </ul>","tags":["ARENA","RL","Bandits","MDPs","documentation"]},{"location":"dlog/2025-10-23/#side-notes","title":"Side Notes","text":"<ul> <li>Might be a nice idea to include writing down the more formalized math down into Latex in a \"knowledge\" section for my own solidifcation and quick reference (like one page phonology tutorials) since my notes are kind of messy and free form</li> </ul>","tags":["ARENA","RL","Bandits","MDPs","documentation"]},{"location":"dlog/2025-10-24/","title":"2025-10-24 | Transformer Lens","text":"<p>Goal: Learn Transformer Lens</p> <p>Summary: Learning Transformer Lens for Encoder-Based Syntax Probe</p> <p>Work sessions</p> In Out 15:30 16:00 16:53 17:55","tags":["Syntax","Transformer Lens","Dyck Language"]},{"location":"dlog/2025-10-24/#dyck-language-probe","title":"Dyck Language Probe","text":"<p>Taking a quick break today on the ARENA curriculum to work on Dyck-Interp-Probe (though it was really fun to start sharing AI Safety ideas/ARENA curriculum with the Computational Linguistics Reading Group yesterday--next week we will start with a Karpathy Zero to Hero to ramp up for ARENA curriculum). </p> <p>The first version which I had worked on was programmed using Pytorch but since we're working on scaling up the experiments (binary searching number of layers needed for high performance and randomizing data partitions), it might be worthwhile to refactor for better collaboration. Super cool to see that Let's build GPT: from scratch, in code, spelled out. was where this project's programming began.</p> <p>This work is done in collaboration with Prof. Khalil Iskarous (Computational Linguistics at USC).</p>","tags":["Syntax","Transformer Lens","Dyck Language"]},{"location":"dlog/2025-10-24/#progress","title":"Progress","text":"<ol> <li> <p>Add support to load from a config rather than specifying in the code (decoupling for better quality of life for experimentation). Also added a logger and saved the training curves.</p> </li> <li> <p>Model performance seems to change throughout different runs, for example:</p> </li> </ol> <pre><code>Run A: Best Epoch = 147 with accuracy = 0.8352941274642944 and unconfidence = 0.12352941185235977\nRun B: Best Epoch = 149 with accuracy = 0.8588235378265381 and unconfidence = 0.10000000149011612\n</code></pre> <ul> <li>The most likely culprits are (1) Data shuffling (2) Model random initialization</li> <li> <p>I am leaning towards (2) being the issue since the data is not shuffled (but will need to verify this)</p> </li> <li> <p>In the experiments, the accuracy is hovering around 85%. Previously, I had seen accuracy close to ceiling (~96% on Validation Set).</p> </li> <li> <p>A follow up task is to tune AdamW optimizer hyperparameters to increase performance</p> </li> <li> <p>Initial experiment from 12 -&gt; 6 layers shows no performance degradation </p> </li> </ul> <pre><code>Run with 6 layers: Best Epoch = 152 with accuracy = 0.8705882430076599 and unconfidence = 0.10588235408067703\n</code></pre>","tags":["Syntax","Transformer Lens","Dyck Language"]},{"location":"dlog/2025-10-25/","title":"2025-10-25 | Applications","text":"<p>Goal: Work on ARENA 7.0 Application Follow Up</p> <p>Summary: Reflect on career goals, start reading Anthropic's Tracing the thoughts of a large language model</p> <p>Work sessions</p> In Out 15:44 16:15 22:36 23:15","tags":["Applications","Documentation"]},{"location":"dlog/2025-10-25/#documentation","title":"Documentation","text":"<ol> <li>Adding StatQuest videos to Todo list</li> <li>I think this will be a good way for me to help follow the concepts covered in ARENA 2.1 Bandits since a lot of the math went over my head!</li> <li>Making a new page for TODOs to not slow the homepage load</li> </ol>","tags":["Applications","Documentation"]},{"location":"dlog/2025-10-26/","title":"2025-10-26 | Applications","text":"<p>Goal: Apply to Research Opportunities</p> <p>Summary: Continue reading Anthropic's Tracing the thoughts of a large language model</p> <p>Work sessions</p> In Out 8:00 9:30 22:07 23:15","tags":["Applications"]},{"location":"dlog/2025-10-27/","title":"2025-10-27 | Stats","text":"<p>Goal: Continue Applications and Start StatQuest journey</p> <p>Summary: Watch StatQuest on Probability/Gaussian Distributions, Uniform Information Density Writeup</p> <p>Work sessions</p> In Out 0:00 1:00 14:15 15:05 15:45 16:10 16:42 17:18","tags":["Applications","Stats","Information Theory"]},{"location":"dlog/2025-10-27/#stats","title":"Stats","text":"<ul> <li>Completed 2 tutorials from StatQuest (Probability distributions and Gaussian Distributions)</li> <li> <p>Add page Stats and Probability Concepts</p> </li> <li> <p>Reference Page: Uniform Information Density Writeup</p> </li> </ul>","tags":["Applications","Stats","Information Theory"]},{"location":"dlog/2025-10-28/","title":"2025-10-28 | Apps","text":"<p>Goal: Continue Application Work!</p> <p>Summary: Looking for research/upskilling opportunities</p> <p>Work sessions</p> In Out 20:09 20:27 <p>Turns out sleep is very important, wasn't able to work for too long but will be back tomorrow.</p>","tags":["Applications"]},{"location":"dlog/2025-10-29/","title":"2025-10-29 | Apps","text":"<p>Goal: Continue Application Work Pt.2</p> <p>Summary: Looking for research/upskilling opportunities</p> <p>Work sessions</p> In Out 13:40 14:10 15:00 16:11 <p>Sleep I did and now I'm back!</p>","tags":["Applications"]},{"location":"dlog/2025-10-30/","title":"2025-10-30 | Apps","text":"<p>Goal: Lead first AI Safety Session!</p> <p>Summary: First time formally introducing AI Safety to Recursion Reading Group; briefly skim Anthropic's Signs of introspection in large language models</p> <p>Work sessions</p> In Out 20:30 21:45","tags":["Recursion Reading Group"]},{"location":"dlog/2025-10-30/#first-ai-safety-meeting","title":"First AI Safety Meeting","text":"<ol> <li> <p>It was nice to see people from different disciplines come together (CS, Cog Sci, Linguistics, Business) united by a shared interest in learning how to build NNs from scratch and captivated by AI Safety (we skimmed Detecting and reducing scheming in AI models)</p> </li> <li> <p>We followed the beginning of Karpathy's Zero to Hero Micrograd but it seemed that this wasn't the best way to engage folks. The reason why the Reading Group was so successful was that we were talking face-to-face and the main point of meeting was discussing complex ideas, not exactly watching a static pre-recorded video.</p> </li> <li> <p>Next week, we will try prepping a visualization-based, first-principle curriculum for staring work on NNs. Perhaps using this previous writeup could be a good start! Interpeting LLM Arithemtics Deep Dive</p> </li> </ol>","tags":["Recursion Reading Group"]},{"location":"dlog/2025-10-31/","title":"2025-10-31 | Stats","text":"<p>Goal: No Zero Days</p> <p>Summary: Mean, Variance, Std Deviation [StatQuest Video]</p> <p>Work sessions</p> In Out 16:53 17:00","tags":["Stats"]},{"location":"dlog/2025-10-31/#meaning-of-no-zero-days","title":"Meaning of No Zero Days","text":"<p>Even on the days where it seems impossible to make time for working on interpretability, I'm going to spend a little bit of time making progress! \u5805\u6301/\u575a\u6301\uff08ji\u00e1n ch\u01d0\uff09or perserverance towards a goal!</p> <p>Today was working on a bit of stats, tomorrow I will revisit this.</p>","tags":["Stats"]},{"location":"notes/References/","title":"Reference Links","text":"<p>Adding links I run into that I think can be helpful later in the journey</p>"},{"location":"notes/References/#ai-safety","title":"AI Safety","text":"<ol> <li>Neel Nanda's Glossary</li> </ol>"},{"location":"notes/References/#math","title":"Math","text":"<ol> <li> <p>StatQuest with Josh Starmer; see StatQuest Statistics Lessons</p> </li> <li> <p>Resources Section from CSCI-467 Introduction to Machine Learning</p> <ul> <li>I took this class in Fall 2024 with Prof. Jieyu Zhao with the course designed and resources compiled by Prof. Robin Jia. Big thanks to both of them for their guidance!</li> </ul> </li> </ol>"},{"location":"notes/References/#math-concepts","title":"Math Concepts","text":"<ul> <li>Probability and Statistics<ul> <li>Probability Distributions [StatQuest Video]</li> <li>Normal Distributions [StatQuest Video]</li> <li>Mean, Variance, Std Deviation [StatQuest Video]</li> <li>Mathematical Models [StatQuest Video]</li> <li>Covariance [StatQuest Video]</li> <li>Conditional Probabilities [StatQuest Video]</li> <li>Bayes Theorem [StatQuest Video]</li> <li>Expected Value <ul> <li>Discrete [StatQuest Video]</li> <li>Continuous [StatQuest Video]</li> </ul> </li> <li>Central Limit Theorem [StatQuest Video]</li> </ul> </li> </ul>"},{"location":"notes/Todo/","title":"Running Todo List","text":""},{"location":"notes/Todo/#math","title":"Math","text":"<ul> <li> Go through StatQuest Statistics Lessons<ul> <li> [Completed 2025-10-27] [Added 2025-10-25] Probability Distributions [StatQuest Video]</li> <li> [Completed 2025-10-27] [Added 2025-10-25] Normal Distributions [StatQuest Video]</li> <li> [Added 2025-10-25] Mean, Variance, Std Deviation [StatQuest Video]</li> <li> [Added 2025-10-25] Mathematical Models [StatQuest Video]</li> <li> [Added 2025-10-25] Covariance [StatQuest Video]</li> <li> [Added 2025-10-25] Conditional Probabilities [StatQuest Video]</li> <li> [Added 2025-10-25] Bayes Theorem [StatQuest Video]</li> <li> [Added 2025-10-25] Expected Value <ul> <li> [Added 2025-10-25] Discrete [StatQuest Video]</li> <li> [Added 2025-10-25] Continuous [StatQuest Video]</li> </ul> </li> <li> [Added 2025-10-25] Central Limit Theorem [StatQuest Video]</li> </ul> </li> </ul>"},{"location":"notes/Todo/#information-theory","title":"Information Theory","text":"<ul> <li> [Added 2025-10-27] Surprisal vs. Perplexity vs. Entropy vs. Cross-Entropy</li> </ul>"},{"location":"notes/Todo/#mechanistic-interpretability","title":"Mechanistic Interpretability","text":"<ul> <li> [Added 2025-10-21] Learn ARENA curriculum</li> </ul>"},{"location":"notes/Todo/#reading","title":"Reading","text":"<ul> <li> [Added 2025-10-26] Download and annotate March 27th, 2025 Circuit Tracing: Revealing Computational Graphs in Language Models<ul> <li> [Added 2025-10-26] Perhaps try to reproduce on a smaller model?</li> <li> Additional reference: [Added 2025-10-26] On the Biology of a Large Language Model</li> </ul> </li> <li> [Added 2025-10-25] Read Tracing the thoughts of a large language model</li> <li> [Added 2025-10-21] Read Progress measures for grokking via mechanistic interpretability by Nanda et al.</li> <li> [Added 2025-10-21] Look into reproducing Progress measures for grokking via mechanistic interpretability by Nanda et al.</li> <li> [Added 2025-10-27] From OpenAI Scheming Blog, Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety</li> </ul>"},{"location":"notes/Todo/#documentation","title":"Documentation","text":"<ul> <li> [Added 2025-10-24] Add section on current project (Dyck Interp Probe)</li> <li> [Completed 25-10-23] Document 25-10-17 to 25-10-20 range of progress</li> </ul>"},{"location":"notes/concepts/stats_and_probability_concepts/","title":"Stats and Probability Concepts","text":""},{"location":"notes/concepts/stats_and_probability_concepts/#distributions","title":"Distributions","text":"Probability Distributions"},{"location":"notes/concepts/stats_and_probability_concepts/#definitions","title":"Definitions","text":"<ul> <li>Histogram: Discrete plot where x-axis is buckets of a measurement (e.g. heights from 5' to 5.5') and y-axis is the count of the number of measurements (e.g. number of people between 5' to 5.5')<ul> <li>In some ways, this is like a C++ array where index is the quantity we want to measure and the value in that index is the frequency count </li> <li>Here we see the discretized bins; we see the distribution show most height are around 5'-6'</li> <li>We would say the probability distribution is centered around the 5'-6' bins and that seeing a 5'-5.5' person has the greatest probability<ul> <li>If we were to pick a person at random, there is the highest probability that that person is from bin 5'-5.5'</li> </ul> </li> </ul> </li> </ul>"},{"location":"notes/papers/information-theory/UID/","title":"Revisting the Uniform Information Density Hypothesis","text":"<p>Authors: Clara Meister, Tiago Pimentel, Patrick Haller, Lena J\u00e4ger, Ryan Cotterell, Roger Levy</p> <p>Publication Date: 2021-09-23</p> <p>Full Paper: Revisiting the Uniform Information Density Hypothesis</p> <ul> <li>See UID as a signal smoothing of effort or a regression to the mean of a targtet information rate</li> <li>Complicated by Global vs local UID (e.g. UID as minimizing word to word variability or total deviation from the mean)</li> </ul>"},{"location":"notes/papers/information-theory/UID/#surprisal","title":"Surprisal","text":"Surprisal <ul> <li>Given a signal \\(\\textbf{u} = \\lang u_1, u_2, ... u_N \\rang\\)</li> <li>\"Surprisal is the negative log prob of the unit conditioned on its prior context\" $$ \\text{surprisal of unit n} = s(u_n) = - \\log p(u_n | \\textbf{u}_{&lt;n}) $$</li> </ul>"},{"location":"notes/papers/information-theory/UID/#effort","title":"Effort","text":"Effort \\[ \\text{Effort} (u_n) \\propto  s(u_n) \\] <ul> <li> <p>Problematic definition below shows that individual surprisal is not incentivized to distribute information in an utterance $$ \\text{Effort} (\\textbf{u}) \\propto  \\sum^N_{n=1} s(u_n) $$</p> </li> <li> <p>This paper proposes a (1) super-linear (greater than linear) function of surprisal which is (2) linear to utterance length $$ \\text{Effort} (\\textbf{u}) \\propto  \\sum^N_{n=1} s(u_n)^k + c \\cdot N$$</p> <p>where $ c &gt; 0 $ and $ k &gt; 1 $</p> </li> </ul>"},{"location":"notes/papers/information-theory/UID/#uid-scope-global-vs-local","title":"UID Scope (Global vs. Local)","text":"<ol> <li>S1 (Red) has better Global UID (relative to a specific mean information rate)<ul> <li>\"as information content per word varies less\u2014in absolute terms\u2014across the sentence\"</li> </ul> </li> <li>S2 (Blue) has better Local UID since there are no big jumps in information rate between consecutive words<ul> <li>\"interpret UID as a pres- sure to avoid rapidly shifting from information dense (and therefore cognitively taxing) sections to sections requiring minimal processing effort\"</li> </ul> </li> </ol> Global UID <ul> <li>UID applied globally to a target mean information rate<ul> <li>Some researchers propose that a universal channel rate exists $$ UID^{-1} = \\frac{1}{N} \\sum^N_{n=2} \\Delta (s(u_n), \\mu _c)$$ </li> </ul> </li> </ul> Local UID \\[ UID^{-1} = \\frac{1}{N-1} \\sum^N_{n=2} \\Delta (s(u_n), s(u_{n-1}))\\]"},{"location":"notes/papers/interpretability/Scheming_OpenAI_Apollo_2025-09-17/","title":"Detecting and reducing scheming in AI models","text":"<p>Authors: OpenAI and Apollo Research</p> <p>Publication Date: 2025-09-17</p> <p>Full Paper: Stress Testing Deliberative Alignment for Anti-Scheming Training</p> <p>Open AI Blog: Detecting and reducing scheming in AI models</p>"},{"location":"notes/papers/interpretability/Scheming_OpenAI_Apollo_2025-09-17/#key-terms","title":"Key Terms","text":"<ul> <li>Scheming<ul> <li>\"Pretending to be aligned while pursuing some other agenda\"</li> <li>From output alone is not possible to see if models are deliberately being unfaithful</li> <li>My definition: Scheming is when an AI model deliberately responds (either correctly or incorrectly) to a user to pursue an unaligned goal.</li> </ul> </li> <li>Covert Actions<ul> <li>\"deliberate witholding or distortion of task-relevant information\"</li> <li>Proxy for scheming </li> <li>When a model makes its answers sound more aligned to a human goal by purposefully distorting its answers</li> </ul> </li> <li>Sandbagging<ul> <li>When a model purposefully underperforms to achieve an unaligned task</li> </ul> </li> <li>Deception<ul> <li>Model pretends to have completed a task without actually doing so <ul> <li>e.g. iconic example of reading a large CSV file and responding before processing</li> </ul> </li> </ul> </li> <li>Situational Awareness<ul> <li>Models become aware that their responses are being evaluated; dangerous for different evaluation- vs. deploy-time behavior</li> </ul> </li> <li>Deliberative Alignment<ul> <li>Teaching models to read and reason through \"anti-scheming\" rules before answering (think of an advanced Isaac Asimov's \"Big Three\" rules of robotics)</li> </ul> </li> </ul>"},{"location":"notes/papers/interpretability/Scheming_OpenAI_Apollo_2025-09-17/#main-takeaways","title":"Main Takeaways","text":"<ol> <li> <p>Reading Chain-of-thought (COT) reasoning transparency in English allows for researchers to peer into model scheming; this would fail if COT is opaque or unfaithful (see Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety)</p> </li> <li> <p>Model Anti-scheming training (through deliberate alignment) can result in either (1) model faithfullness or (2) models become better at scheming/hide misalignment</p> <ul> <li>Deliberate alignment's anti-scheming specification is meant to train model faithfullness (1) not hiding misalignment (2); supported by OpenAI model citing spec in COT reasoning</li> </ul> </li> <li> <p>Situational Awareness may allow for modesl to score well during alignment benchmarks but execute misaligned behavior in production; current impact is limited because models do not have opportunity to cause harm</p> </li> </ol> Always Inserting Situational Awareness Prompt <ul> <li>Interesting finding: Removing Situational Awareness COT increases likelihood for scheming; Adding Situational Awareness COT decreases likelihood for scheming</li> <li>Wonder if it's possible to always just add Situational Awareness in COT for production deployment;(1)perhaps models would start outsmarting this method and (2) does not solve the fundamental alignment problem</li> </ul>"},{"location":"notes/papers/interpretability/Scheming_OpenAI_Apollo_2025-09-17/#reflections","title":"Reflections","text":""},{"location":"notes/papers/interpretability/Scheming_OpenAI_Apollo_2025-09-17/#guiding-questions-from-arena","title":"Guiding Questions from ARENA","text":"<ol> <li>Why might model scheming be a particularly worrying concern as models get more capable? </li> <li>Why does situational awareness (or evaluation awareness) pose a problem for the field of model evaluations? How might we try to mitigate this?</li> <li>In your view, does this paper provide evidence for or against the following suggestion: \"Deliberative alignment will effectively work to address our concerns about scheming in LLMs.\" Justify your opinion.</li> </ol>"}]}