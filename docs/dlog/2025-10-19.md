---
date: 2025-10-19
sessions:
  - {in: "19:00", out: "23:00"}
goal: "Shannon 1948 Discrete Noiseless Channel"
summary: "Multi-disciplinary approach to Interpretability, viewing models through the lens of Entropy"
tags: [Information Theory]
---

# {{ page.meta.date }} | Shannon

**Goal:** {{ page.meta.goal }}

**Summary:** {{ page.meta.summary }}

**Work sessions**

| In   | Out  |
|------|------|
{% for s in page.meta.sessions -%}
| {{ s.in }} | {{ s.out }} |
{% endfor %}

## Why Information Theory
Shannon 1948 is a seminal text on the way scientists/engineers view how computer's transmit, store, and produce information.

While written 77 years ago, concepts like Entropy (in fact, LLMs are trained on [Cross-Entropy loss functions](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)!), Mutual Information, and Noise perhaps may lead to greater insight which emerge from model training pressures.

Nicely intersected with a reading for my [Phonology](https://en.wikipedia.org/wiki/Phonology) Course