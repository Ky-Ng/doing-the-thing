---
date: 2025-12-02
sessions:
  - {in: "14:45", out: "15:50"}
  - {in: "22:45", out: "23:50"}
goal: "Reproduce Neo et al. 2024"
summary: "Fix Tokenizer to use left truncation and padding to keep max activating prompt right context; reach out to AI Safety folks"
tags: [Reproducing Papers, Neo et al. 2024]
---

# {{ page.meta.date }} | Reproducing Neo et al. 

**Goal:** {{ page.meta.goal }}

**Summary:** {{ page.meta.summary }}

**Work sessions**

| In   | Out  |
|------|------|
{% for s in page.meta.sessions -%}
| {{ s.in }} | {{ s.out }} |
{% endfor %}

## Reading List
Added Rohin Shah's [Google DeepMind: An Approach to Technical AGI Safety and Security](https://www.alignmentforum.org/posts/3ki4mt4BA6eTx56Tc/google-deepmind-an-approach-to-technical-agi-safety-and) to [reading list](../notes/Todo.md)