---
date: 2025-10-30
sessions:
  - {in: "20:30", out: "21:45"}
goal: "Lead first AI Safety Session!"
summary: "First time formally introducing AI Safety to Recursion Reading Group; briefly skim Anthropic's [Signs of introspection in large language models](https://www.anthropic.com/research/introspection)"
tags: [Recursion Reading Group]
---

# {{ page.meta.date }} | Apps

**Goal:** {{ page.meta.goal }}

**Summary:** {{ page.meta.summary }}

**Work sessions**

| In   | Out  |
|------|------|
{% for s in page.meta.sessions -%}
| {{ s.in }} | {{ s.out }} |
{% endfor %}

## First AI Safety Meeting
1. It was nice to see people from different disciplines come together (CS, Cog Sci, Linguistics, Business) united by a shared interest in learning how to build NNs from scratch and captivated by AI Safety (we skimmed [Detecting and reducing scheming in AI models](https://openai.com/index/detecting-and-reducing-scheming-in-ai-models/))

2. We followed the beginning of Karpathy's Zero to Hero Micrograd but it seemed that this wasn't the best way to engage folks. The reason why the Reading Group was so successful was that we were talking face-to-face and the main point of meeting was discussing complex ideas, not exactly watching a static pre-recorded video.

3. Next week, we will try prepping a visualization-based, first-principle curriculum for staring work on NNs. Perhaps using this previous writeup could be a good start! [Interpeting LLM Arithemtics Deep Dive](https://github.com/Ky-Ng/Dyck-Interp-Probe/blob/main/Interpeting_LLM_Arithmetics_Deep_Dive_Notes.pdf)