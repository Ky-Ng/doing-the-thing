---
date: 2025-11-05
sessions:
  - {in: "12:30", out: "13:15"}
  - {in: "15:00", out: "15:20"}
goal: "Read 80,000 hours career guide"
summary: "Apply for 80,000 hour mentorship and watched Robert Miles AI Safety career video"
tags: [Reflection]
---

# {{ page.meta.date }} | Career

**Goal:** {{ page.meta.goal }}

**Summary:** {{ page.meta.summary }}

**Work sessions**

| In   | Out  |
|------|------|
{% for s in page.meta.sessions -%}
| {{ s.in }} | {{ s.out }} |
{% endfor %}

## Career Reflection
- Found [Map of AI Existential Safety](https://www.aisafety.com/map) from AISafety.com through ARENA Slack
- Watched [AI Safety Career Advice! (And So Can You!)](https://www.youtube.com/watch?v=OpufM6yK4Go) by Robert Miles AI Safety
- Read [Dream Job Blogpost](https://80000hours.org/career-guide/job-satisfaction/) from 80,000 Hours
- Applied for 1-on-1 coaching session with 80,000 Hours

## Direction of Doing the Thing
- Something I realized as Finals are approaching and I've been spending lots of time reading linguistics papers is that I have not been reading AI Safety papers as much!
- While I think it is good for me to continue to work on/brush up on the prerequisite knowledge in [Running Todo List](../notes/Todo.md) I want to start taking greater action in `doing the thing`.
  - One way to take more action is perhaps to start trying to implement papers and DFS through concepts when finding them as blockers to achieving the next level of understanding. Since reproducing a paper is a finite goal and AI tools can help make structured plans, I think this can be a great way to start getting my hands dirty.

### Fazl Barez Research on Automatic Interpretability
- Found some pretty interesting work by [Fazl Barez](https://fbarez.github.io) on using LLMs to automate analysis of circuits: 
- Wish I was at this [AI Safety and AI Alignment bootcamp](https://fbarez.github.io/AISAA/)
