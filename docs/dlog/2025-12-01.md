---
date: 2025-12-01
sessions:
  - {in: "00:00", out: "00:35"}
  - {in: "15:30", out: "16:30"}
goal: "Reproduce Neo et al. 2024"
summary: "Allow tokenizer to process `Batch Size > 1`"
tags: [Reproducing Papers, Neo et al. 2024]
---

# {{ page.meta.date }} | Reproducing Neo et al.

**Goal:** {{ page.meta.goal }}

**Summary:** {{ page.meta.summary }}

**Work sessions**

| In   | Out  |
|------|------|
{% for s in page.meta.sessions -%}
| {{ s.in }} | {{ s.out }} |
{% endfor %}

## Reproducing Neo et al.
1. Allow tokenizer to have multiple batches
???+ abstract "Tokenizer Padding ID"  
    - `padding`: GPT-2 does not have a padding token by default
    - instead, use the end-of-sequence (EOS) token instead
    ```py
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id
    ```

    Thus, when tokenizing two prompts, the attention mask/padding will extend prompts to the longest prompt in the batch
    ```py
    "hi there" --> [5303, 612, 50256, 50256, 50256] # See that 50256 is the padding token
    "what time is it?" --> [10919, 640, 318, 340, 30]
    ```

    Therefore, we see the attention mask 0-ed out for `hi there`
    ```py
    "hi there" --> [1, 1, 0, 0, 0]
    "what time is it?" --> [1, 1, 1, 1, 1]
    ```
2. Right-truncate long prompts with `truncation=True` in `tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)`
3. OOM (Out of Memory) issues with high batch size inference
    - Solution: support using Collab with VsCode to add GPU Support
    - Ensure that tensors are on the same device during computation 
    - Results: 4 min = 4000 examples vs. before 2000

## Reading List
Add [How Can Interpretability Researchers Help AGI Go Well?](https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well) to reading list

## Next Steps
1. finish section 4.2 activating neurons
2. Write out the prompts to a serialized form (e.g. JSON/CSV) 
3. Truncate prompts to be 80% activation only